{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# change default directory\n",
    "os.chdir(r\"C:\\Users\\ngwei\\Dropbox\\UM_Master\\UM_Sem3\\WQD7005_Data_Mining\\WQD7005_Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import json\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "def tweet_cleaner(text):\n",
    "    rep_and = str(text).lower().replace('&',\"and\")\n",
    "    stripped = rep_and.replace('\\n\\n',\" \")\n",
    "    stripped_1 = stripped.replace('\\n',\" \")\n",
    "    stripped_2= stripped_1.replace(u\"\\\\u201c\", \"\")\n",
    "    replace_key = stripped_2.replace('#covid19','coronavirus')\n",
    "    strip_link = re.sub('https?://[A-Za-z0-9./]+','',replace_key)\n",
    "    no_hash = re.sub(r'@[A-Za-z0-9]+','',strip_link)\n",
    "    no_tag = re.sub(r'#[A-Za-z0-9]+','',no_hash)\n",
    "    no_slash = re.sub(r'\\\\[A-Za-z0-9]+','',no_tag)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", no_slash)\n",
    "    remove_word = re.sub(r'\\b\\w{1}\\b', '', letters_only)\n",
    "    words = tok.tokenize(remove_word)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(text):\n",
    "    text = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def prepare_text_lda(text):\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def getSentiment(df_clean):\n",
    "    \n",
    "    # Calculate polarity of each review\n",
    "    sentimentAnalyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = []\n",
    "    sentiment_index = []\n",
    "    \n",
    "    for review in df_clean['stemming_text']:\n",
    "        \n",
    "        ss = sentimentAnalyzer.polarity_scores(review)\n",
    "        \n",
    "        if ss['compound'] >= 0.05 : \n",
    "            # print(\"Positive\") \n",
    "            sentiment.append(\"Positive\")\n",
    "            sentiment_index.append(1)\n",
    "        elif ss['compound'] <= -0.05 : \n",
    "            # print(\"Negative\") \n",
    "            sentiment.append(\"Negative\")\n",
    "            sentiment_index.append(-1)\n",
    "        else : \n",
    "            # print(\"Neutral\") \n",
    "            sentiment.append(\"Neutral\")\n",
    "            sentiment_index.append(0)\n",
    "    \n",
    "    df_clean[\"sentiment\"] = sentiment\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with TF-IDF vectorizer\n",
    "def LogReg(): \n",
    "    start_time = time.time()\n",
    "    logreg = LogisticRegression(C=0.1, solver='lbfgs', multi_class='auto', max_iter=3000).fit(X_train, y_train)\n",
    "    log_y_pred = logreg.predict(X_test)\n",
    "    log_accuracy = accuracy_score(y_test, log_y_pred)\n",
    "    log_precision = precision_score(y_test, log_y_pred, average='macro')\n",
    "    log_recall = recall_score(y_test, log_y_pred, average='macro')\n",
    "    log_f1 = f1_score(y_test, log_y_pred, average='macro')\n",
    "    log_time = time.time() - start_time\n",
    "    return ['Logistic Regression', log_accuracy, log_precision, log_recall, log_f1, log_time]\n",
    "\n",
    "# Decision Tree with TF-IDF vectorizer\n",
    "def DecTree(): \n",
    "    start_time = time.time()\n",
    "    decision_tree = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    decisionTree_y_pred = decision_tree.predict(X_test)\n",
    "    decisionTree_accuracy = accuracy_score(y_test, decisionTree_y_pred)\n",
    "    decisionTree_precision = precision_score(y_test, decisionTree_y_pred, average='macro')\n",
    "    decisionTree_recall = recall_score(y_test, decisionTree_y_pred, average='macro')\n",
    "    decisionTree_f1 = f1_score(y_test, decisionTree_y_pred, average='macro')\n",
    "    decisionTree_time = time.time() - start_time\n",
    "    return ['Decision Tree', decisionTree_accuracy, decisionTree_precision, decisionTree_recall, decisionTree_f1, decisionTree_time]\n",
    "\n",
    "\n",
    "# Random Forest with TF-IDF vectorizer\n",
    "def RandomForest():\n",
    "    start_time = time.time()\n",
    "    random_forest = RandomForestClassifier(n_estimators = 100).fit(X_train, y_train)\n",
    "    rf_pred = random_forest.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "    rf_precision = precision_score(y_test, rf_pred, average='macro')\n",
    "    rf_recall = recall_score(y_test, rf_pred, average='macro')\n",
    "    rf_f1 = f1_score(y_test, rf_pred, average='macro')\n",
    "    rf_time = time.time() - start_time\n",
    "    return ['Random Forest', rf_accuracy, rf_precision, rf_recall, rf_f1, rf_time]\n",
    "\n",
    "\n",
    "# Naive Bayes with TF-IDF vectorizer\n",
    "def NaiveBayes():\n",
    "    start_time = time.time()\n",
    "    multinomialNB = MultinomialNB().fit(X_train, y_train)\n",
    "    multinomialNB_pred = multinomialNB.predict(X_test)\n",
    "    multinomialNB_accuracy = accuracy_score(y_test, multinomialNB_pred)\n",
    "    multinomialNB_precision = precision_score(y_test, multinomialNB_pred, average='macro')\n",
    "    multinomialNB_recall = recall_score(y_test, multinomialNB_pred, average='macro')\n",
    "    multinomialNB_f1 = f1_score(y_test, multinomialNB_pred, average='macro')\n",
    "    multinomialNB_time = time.time() - start_time\n",
    "    return ['Multinomial Naive Bayes', multinomialNB_accuracy, multinomialNB_precision, multinomialNB_recall, multinomialNB_f1, multinomialNB_time]\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost with TF-IDF vectorizer\n",
    "def xgBoost():\n",
    "    start_time = time.time()\n",
    "    xgb = XGBClassifier().fit(X_train, y_train)\n",
    "    xgb_y_pred = xgb.predict(X_test)\n",
    "    xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "    xgb_precision = precision_score(y_test, xgb_y_pred, average='macro')\n",
    "    xgb_recall = recall_score(y_test, xgb_y_pred, average='macro')\n",
    "    xgb_f1 = f1_score(y_test, xgb_y_pred, average='macro')\n",
    "    xgb_time = time.time() - start_time\n",
    "    return ['XG Boost', xgb_accuracy, xgb_precision, xgb_recall, xgb_f1, xgb_time]\n",
    "    \n",
    "\n",
    "    \n",
    "# SVM with TF-IDF vectorizer\n",
    "def svmLinear():\n",
    "    start_time = time.time()\n",
    "    svm = SVC(gamma='auto', kernel='linear').fit(X_train, y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "    svm_precision = precision_score(y_test, svm_pred, average='macro')\n",
    "    svm_recall = recall_score(y_test, svm_pred, average='macro')\n",
    "    svm_f1 = f1_score(y_test, svm_pred, average='macro')\n",
    "    svm_time = time.time() - start_time\n",
    "    return ['SVM (Linear)', svm_accuracy, svm_precision, svm_recall, svm_f1, svm_time]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: \n",
    "\n",
    "1. Flask + Charts: https://blog.ruanbekker.com/blog/2017/12/14/graphing-pretty-charts-with-python-flask-and-chartjs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:18] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:20] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:24] \"GET /randomforest HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:25] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:27] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:28] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:29] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:31] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:32:32] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:03] \"GET /logreg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:05] \"GET /logreg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:05] \"GET /logreg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:07] \"GET /naivebayes HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:10] \"GET /xgboost HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:15] \"GET /svmlinear HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:18] \"GET /decisiontree HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jun/2020 12:33:20] \"GET /sentiment HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import render_template\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "\n",
    "## Pre-load data and run text-processing\n",
    "df_tmp = pd.read_csv('dataset/rawdata.csv', index_col=None, header=0)\n",
    "df_tmp['full_text'] = df_tmp['full_text'].apply(tweet_cleaner)\n",
    "df_tmp['lemmatize_text'] = df_tmp['full_text'].apply(lambda x: lemmatize_sentence(x))\n",
    "df_tmp['stemming_text'] = df_tmp['lemmatize_text'].apply(lambda x: stemming(x))\n",
    "df_tmp = getSentiment(df_tmp)\n",
    "\n",
    "\n",
    "## Preparing for ML Algorithms\n",
    "# Split into testing set and training set\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_tmp['stemming_text'], df_tmp['sentiment'], random_state=0)\n",
    "# Vectorize X_train\n",
    "vectorizer = TfidfVectorizer(min_df = 5).fit(x_train)\n",
    "X_train = vectorizer.transform(x_train)\n",
    "X_test = vectorizer.transform(x_test)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "## Home Tab\n",
    "\n",
    "@app.route('/')\n",
    "@app.route('/index')\n",
    "def myIndex():\n",
    "    postTitle = \"WQD7005 Flask Deployment\"\n",
    "    posts = [\n",
    "        {'Name': 'Project Description', 'Content': 'Sentiment Analysis using social media data from Twitter'},\n",
    "        {'Name': 'Project Description', 'Content': 'Dataset has been loaded, click from menus to check things out!'}\n",
    "    ]\n",
    "    return render_template('index.html', title='Home', postTitle=postTitle, posts=posts)\n",
    "\n",
    "\n",
    "## Sentiment Analysis Tab\n",
    "\n",
    "@app.route('/sentiment')\n",
    "def mySentiment():\n",
    "    \n",
    "    postTitle = \"Sentiment Analysis on Tweets Data about Donald Trump:\"\n",
    "    tmpData = [{'Name': i, 'Content': str(len(df_tmp[df_tmp['sentiment']==i]))} for i in set(df_tmp['sentiment'])]\n",
    "    pieValues = [str(len(df_tmp[df_tmp['sentiment']==i])) for i in set(df_tmp['sentiment'])]\n",
    "    pieLabels = set(df_tmp['sentiment'])\n",
    "    pieColours = [\"#F7464A\", \"#46BFBD\", \"#FDB45C\"]\n",
    "    return render_template('sentiment.html', title='Sentiment', postTitle=postTitle, posts=tmpData, max=17000, set=zip(pieValues, pieLabels, pieColours))\n",
    "\n",
    "\n",
    "## Machine Learning Tabs\n",
    "\n",
    "## Logistic Regression\n",
    "@app.route('/logreg')\n",
    "def myLogReg():\n",
    "    \n",
    "    postTitle = \"Logistic Regression\"\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Time (s)']\n",
    "    LogRegValues = LogReg()\n",
    "    tmpData = [{'Name': columns[i], 'Content': LogRegValues[i]} for i in range(len(columns))]\n",
    "    return render_template('logreg.html', title='Sentiment', postTitle=postTitle, posts=tmpData)\n",
    "\n",
    "## Decision Tree\n",
    "@app.route('/decisiontree')\n",
    "def myDecTree():\n",
    "    \n",
    "    postTitle = \"Decision Tree\"\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Time (s)']\n",
    "    DecTreeValues = DecTree()\n",
    "    tmpData = [{'Name': columns[i], 'Content': DecTreeValues[i]} for i in range(len(columns))]\n",
    "    return render_template('decisiontree.html', title='Sentiment', postTitle=postTitle, posts=tmpData)\n",
    "\n",
    "## Random Forest\n",
    "@app.route('/randomforest')\n",
    "def myRandomForest():\n",
    "    \n",
    "    postTitle = \"Random Forest:\"\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Time (s)']\n",
    "    rfValues = RandomForest()\n",
    "    tmpData = [{'Name': columns[i], 'Content': rfValues[i]} for i in range(len(columns))]\n",
    "    return render_template('randomforest.html', title='Sentiment', postTitle=postTitle, posts=tmpData)\n",
    "\n",
    "## Naive Bayes\n",
    "@app.route('/naivebayes')\n",
    "def myNB():\n",
    "    \n",
    "    postTitle = \"Naive Bayes:\"\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Time (s)']\n",
    "    nbValues = NaiveBayes()\n",
    "    tmpData = [{'Name': columns[i], 'Content': nbValues[i]} for i in range(len(columns))]\n",
    "    return render_template('naivebayes.html', title='Sentiment', postTitle=postTitle, posts=tmpData)\n",
    "\n",
    "## XG Boost\n",
    "@app.route('/xgboost')\n",
    "def myXGB():\n",
    "    \n",
    "    postTitle = \"XG Boost:\"\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Time (s)']\n",
    "    xgbValues = xgBoost()\n",
    "    tmpData = [{'Name': columns[i], 'Content': xgbValues[i]} for i in range(len(columns))]\n",
    "    return render_template('xgboost.html', title='Sentiment', postTitle=postTitle, posts=tmpData)\n",
    "\n",
    "## SVM (Linear)\n",
    "@app.route('/svmlinear')\n",
    "def mySVM():\n",
    "    \n",
    "    postTitle = \"SVM (Linear)\"\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Time (s)']\n",
    "    svmValues = svmLinear()\n",
    "    tmpData = [{'Name': columns[i], 'Content': svmValues[i]} for i in range(len(columns))]\n",
    "    return render_template('svmlinear.html', title='Sentiment', postTitle=postTitle, posts=tmpData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
