{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 1: Web Scrapping\n",
    "WQD180102 Ng Wei Xin <br>\n",
    "WQD180104 Tan Bing Shien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests\n",
    "# pip install beautifulsoup4\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# using requests to fetch pages\n",
    "# Note: Python Selenium library can be used for javascript pages (by imitating browser behaviour)\n",
    "\n",
    "# bs4: Html-Parser (or XML), to navigate through html documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get url content using python-requests\n",
    "page = requests.get('https://www.thestar.com.my/news/latest')\n",
    "contents = page.content\n",
    "\n",
    "# Parse html document in bs4\n",
    "soup = BeautifulSoup(contents, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dict\n",
    "# Note: using news-id as identifier -> prevent duplication\n",
    "myList = {}\n",
    "\n",
    "# Use class \"timeline-content\" to identify news section\n",
    "for tag in soup.find_all(class_= \"timeline-content\", limit=15):\n",
    "\n",
    "    # Further navigate to \"h2\" tag\n",
    "    for subtag in tag.find('h2'):\n",
    "        \n",
    "        # Get String (news headline), trim / strip linebreaks\n",
    "        strNews = \"\".join(line.strip() for line in subtag.string.split(\"\\n\"))\n",
    "        idNews = \"\".join(line.strip() for line in subtag['data-content-id'].split(\"\\n\"))\n",
    "        \n",
    "        # Crawl sub page, get Date / Timestamp\n",
    "        newsHref = subtag['href']\n",
    "        subPage = requests.get(newsHref)\n",
    "        subContent = subPage.content\n",
    "        subSoup = BeautifulSoup(subContent, 'html.parser')\n",
    "        timeTag = \"\".join(line.strip() for line in subSoup.find('time', {'class': 'timestamp'}).string.split(\"\\n\"))\n",
    "        dateTag = \"\".join(line.strip() for line in subSoup.find('p', {'class': 'date'}).string.split(\"\\n\"))\n",
    "        \n",
    "        # (print) News-ID: News-String\n",
    "        print(\"News-ID (\"+idNews+\"):\")\n",
    "        print(strNews)\n",
    "        print(newsHref)\n",
    "        print(dateTag)\n",
    "        print(timeTag)\n",
    "        print()\n",
    "        \n",
    "        # Add entry to list \n",
    "        myList[subtag['data-content-id']]=strNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recrawl(objDF):\n",
    "    page = requests.get('https://www.thestar.com.my/news/latest')\n",
    "    contents = page.content\n",
    "\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "    for tag in soup.find_all(class_= \"timeline-content\", limit = 15):\n",
    "        for subtag in tag.find('h2'):\n",
    "            \n",
    "            # News-ID\n",
    "            idNews = \"\".join(line.strip() for line in subtag['data-content-id'].split(\"\\n\"))\n",
    "                        \n",
    "            # Check for duplication, proceed only for new item\n",
    "            # else skip to save computation\n",
    "            if not df['News_ID'].str.contains(idNews).any():\n",
    "                \n",
    "                # Headline\n",
    "                strNews = \"\".join(line.strip() for line in subtag.string.split(\"\\n\"))\n",
    "                \n",
    "                # Category\n",
    "                catNews = \"\".join(line.strip() for line in subtag['data-content-category'].split(\"\\n\"))\n",
    "                \n",
    "                # Timestamp (spider on respective news page)\n",
    "                newsHref = subtag['href']\n",
    "                subPage = requests.get(newsHref)\n",
    "                subContent = subPage.content\n",
    "                subSoup = BeautifulSoup(subContent, 'html.parser')\n",
    "                dateNews = \"\".join(line.strip() for line in subSoup.find(class_=\"date\").string.split(\"\\n\"))\n",
    "                timeNews = \"\".join(line.strip() for line in subSoup.find(class_=\"timestamp\").string.split(\"\\n\"))\n",
    "                \n",
    "                # Add info to list\n",
    "                objDF = objDF.append(pd.Series([idNews, dateNews, timeNews, catNews, strNews, newsHref], index=objDF.columns), ignore_index=True)\n",
    "\n",
    "    return objDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty DataFrame:\n",
    "df= pd.DataFrame(columns=[\"News_ID\", \"Date\", \"Timestamp\", \"Category\", \"Headline\", \"URL\"])\n",
    "\n",
    "# Endless-Loop\n",
    "# while True: \n",
    "    \n",
    "i = 1 # Test: using 3 counts \n",
    "while i>0:\n",
    "    \n",
    "    df = recrawl(df)\n",
    "    print(df)\n",
    "    \n",
    "    # Export dataframe to csv file\n",
    "    df.to_csv(r'C:\\Users\\FORGE-15 I7\\OneDrive - AsiaPay Limited\\Sem 3\\WQD7005 DATA MINING\\dataset.csv', index = False)\n",
    "#     df.to_csv(r'C:\\Users\\ngwei\\Desktop\\dataset.csv', index = False)\n",
    "    \n",
    "    i=i-1\n",
    "    #time.sleep(300) # Re-Crawl every 5 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# df.to_csv(r'C:\\Users\\ngwei\\Desktop\\dataset.csv', index = False)\n",
    "\n",
    "\n",
    "# # Save scraped content into a txt file\n",
    "\n",
    "# file1 = open(\"tmpcrawl.txt\",\"w\") \n",
    "\n",
    "# # file1.write(\"Hello \\n\") \n",
    "# # file1.write(str(soup.prettify())) \n",
    "# file1.write(str(myList)) \n",
    "\n",
    "# file1.close() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
